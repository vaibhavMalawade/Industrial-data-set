{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(data, target_column_name, test_size=0.2, val_size=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits the dataset into features and target variables, then further splits into \n",
    "    training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "    data (pd.DataFrame): The dataset to split.\n",
    "    target_column_name (str): The name of the target column.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    val_size (float): Proportion of the dataset to include in the validation split.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before \n",
    "                                  applying the split. Pass an int for reproducible output.\n",
    "    \n",
    "    Returns:\n",
    "    X_train (pd.DataFrame): Training features.\n",
    "    X_val (pd.DataFrame): Validation features.\n",
    "    X_test (pd.DataFrame): Test features.\n",
    "    y_train (pd.Series): Training target variable.\n",
    "    y_val (pd.Series): Validation target variable.\n",
    "    y_test (pd.Series): Test target variable.\n",
    "    \"\"\"\n",
    "    # Ensure the target column is in the dataframe\n",
    "    if target_column_name not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target_column_name}' not found in dataset.\")\n",
    "    \n",
    "    # Split the dataset into features and target\n",
    "    X = data.drop(columns=[target_column_name])\n",
    "    y = data[target_column_name]\n",
    "    \n",
    "    # First, split into train and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    \n",
    "    # Calculate the proportion of validation set with respect to the temporary set (remaining data after train split)\n",
    "    val_proportion = val_size / (1 - test_size)\n",
    "    \n",
    "    # Split the temporary set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_size, random_state=random_state, stratify=y_temp)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame `df` and the target column is 'target'\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df, 'target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_result(model_name, score):\n",
    "    test_result[model_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'E:\\Project Thesis\\Data set\\final\\Notebook\\data.csv')\n",
    "target_column_name = 'quality'\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df ,target_column_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160, 13)\n",
      "(232, 13)\n",
      "(59, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= y_train - 1\n",
    "y_val= y_val - 1\n",
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune_model(X_train, y_train, X_val, y_val, classifier_name):\n",
    "    def objective(trial):\n",
    "        if classifier_name == \"K-Neighbors Classifier\":\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        elif classifier_name == \"Decision Tree\":\n",
    "            max_depth = trial.suggest_int('max_depth', 1, 50)\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        elif classifier_name == \"Random Forest Classifier\":\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "            max_depth = trial.suggest_int('max_depth', 1, 50)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "        elif classifier_name == \"XGBClassifier\":\n",
    "            ''' n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "            max_depth = trial.suggest_int('max_depth', 2, 15),\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,tree_method='gpu_hist', gpu_id=0, use_label_encoder=False)'''\n",
    "            \n",
    "            param = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
    "            'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
    "            'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
    "            'reg_alpha': trial.suggest_int('reg_alpha', 1, 50),\n",
    "            'reg_lambda': trial.suggest_int('reg_lambda', 5, 100),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "             } \n",
    "            model = XGBClassifier(random_state=42, \n",
    "                             tree_method='hist', \n",
    "                             \n",
    "                             predictor=\"cpu_predictor\"\n",
    "                             ,**param ) \n",
    "            \n",
    "        elif classifier_name == \"CatBoostClassifier\":\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "            depth = trial.suggest_int('depth', 1, 10)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "            model = CatBoostClassifier(n_estimators=n_estimators, depth=depth, learning_rate=learning_rate, task_type='GPU', verbose=0)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown classifier: {}\".format(classifier_name))\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    if classifier_name == \"K-Neighbors Classifier\":\n",
    "        model = KNeighborsClassifier(**best_params)\n",
    "    elif classifier_name == \"Decision Tree\":\n",
    "        model = DecisionTreeClassifier(**best_params)\n",
    "    elif classifier_name == \"Random Forest Classifier\":\n",
    "        model = RandomForestClassifier(**best_params)\n",
    "    elif classifier_name == \"XGBClassifier\":\n",
    "        model = XGBClassifier(**best_params,tree_method='hist')\n",
    "    elif classifier_name == \"CatBoostClassifier\":\n",
    "        model = CatBoostClassifier(**best_params,task_type='GPU',devices='0', verbose=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 13:03:31,505] A new study created in memory with name: no-name-82ac96a1-ca38-4f9d-b7ba-5d8e1e1ca4a4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:03:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:04:01,018] Trial 0 finished with value: 0.9310344827586207 and parameters: {'max_depth': 7, 'subsample': 0.7, 'n_estimators': 5900, 'eta': 0.08, 'reg_alpha': 26, 'reg_lambda': 85, 'min_child_weight': 2, 'colsample_bytree': 0.8337716129627449}. Best is trial 0 with value: 0.9310344827586207.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:04:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:04:18,639] Trial 1 finished with value: 0.9267241379310345 and parameters: {'max_depth': 13, 'subsample': 0.95, 'n_estimators': 3600, 'eta': 0.02, 'reg_alpha': 23, 'reg_lambda': 11, 'min_child_weight': 10, 'colsample_bytree': 0.9448720956806842}. Best is trial 0 with value: 0.9310344827586207.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:04:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:04:47,810] Trial 2 finished with value: 0.9224137931034483 and parameters: {'max_depth': 4, 'subsample': 0.75, 'n_estimators': 6800, 'eta': 0.09999999999999999, 'reg_alpha': 29, 'reg_lambda': 100, 'min_child_weight': 10, 'colsample_bytree': 0.501747500227355}. Best is trial 0 with value: 0.9310344827586207.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:04:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:04,236] Trial 3 finished with value: 0.9482758620689655 and parameters: {'max_depth': 6, 'subsample': 0.95, 'n_estimators': 3000, 'eta': 0.05, 'reg_alpha': 5, 'reg_lambda': 19, 'min_child_weight': 6, 'colsample_bytree': 0.6699718653592507}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:10,524] Trial 4 finished with value: 0.9353448275862069 and parameters: {'max_depth': 9, 'subsample': 0.9, 'n_estimators': 1200, 'eta': 0.08, 'reg_alpha': 19, 'reg_lambda': 50, 'min_child_weight': 7, 'colsample_bytree': 0.3492099939974184}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:26,510] Trial 5 finished with value: 0.9094827586206896 and parameters: {'max_depth': 3, 'subsample': 0.75, 'n_estimators': 3500, 'eta': 0.09999999999999999, 'reg_alpha': 32, 'reg_lambda': 9, 'min_child_weight': 15, 'colsample_bytree': 0.11651402805669435}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:31,758] Trial 6 finished with value: 0.9224137931034483 and parameters: {'max_depth': 15, 'subsample': 0.9, 'n_estimators': 1000, 'eta': 0.08, 'reg_alpha': 32, 'reg_lambda': 44, 'min_child_weight': 3, 'colsample_bytree': 0.7025943286760934}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:36,552] Trial 7 finished with value: 0.9051724137931034 and parameters: {'max_depth': 4, 'subsample': 0.95, 'n_estimators': 1000, 'eta': 0.06999999999999999, 'reg_alpha': 48, 'reg_lambda': 59, 'min_child_weight': 9, 'colsample_bytree': 0.46100551293758185}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:43,411] Trial 8 finished with value: 0.9267241379310345 and parameters: {'max_depth': 9, 'subsample': 0.65, 'n_estimators': 1000, 'eta': 0.04, 'reg_alpha': 18, 'reg_lambda': 97, 'min_child_weight': 6, 'colsample_bytree': 0.5982909254812382}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:05:50,937] Trial 9 finished with value: 0.9310344827586207 and parameters: {'max_depth': 12, 'subsample': 0.8, 'n_estimators': 1100, 'eta': 0.04, 'reg_alpha': 15, 'reg_lambda': 27, 'min_child_weight': 11, 'colsample_bytree': 0.6504468291135849}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:05:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:06:37,467] Trial 10 finished with value: 0.9267241379310345 and parameters: {'max_depth': 6, 'subsample': 1.0, 'n_estimators': 9400, 'eta': 0.02, 'reg_alpha': 3, 'reg_lambda': 30, 'min_child_weight': 20, 'colsample_bytree': 0.2307596338939416}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:06:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:06:59,702] Trial 11 finished with value: 0.9353448275862069 and parameters: {'max_depth': 9, 'subsample': 0.85, 'n_estimators': 3400, 'eta': 0.060000000000000005, 'reg_alpha': 4, 'reg_lambda': 57, 'min_child_weight': 7, 'colsample_bytree': 0.33496845899649924}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:06:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:07:15,387] Trial 12 finished with value: 0.9310344827586207 and parameters: {'max_depth': 10, 'subsample': 0.9, 'n_estimators': 3100, 'eta': 0.05, 'reg_alpha': 10, 'reg_lambda': 77, 'min_child_weight': 5, 'colsample_bytree': 0.35947821164136057}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:07:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:07:27,921] Trial 13 finished with value: 0.9310344827586207 and parameters: {'max_depth': 7, 'subsample': 1.0, 'n_estimators': 2600, 'eta': 0.08, 'reg_alpha': 11, 'reg_lambda': 38, 'min_child_weight': 7, 'colsample_bytree': 0.776939671687755}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:07:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:07:48,269] Trial 14 finished with value: 0.9094827586206896 and parameters: {'max_depth': 11, 'subsample': 0.85, 'n_estimators': 4800, 'eta': 0.060000000000000005, 'reg_alpha': 43, 'reg_lambda': 70, 'min_child_weight': 14, 'colsample_bytree': 0.3768971916462518}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:07:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:08:12,049] Trial 15 finished with value: 0.9267241379310345 and parameters: {'max_depth': 5, 'subsample': 0.6, 'n_estimators': 2400, 'eta': 0.04, 'reg_alpha': 1, 'reg_lambda': 20, 'min_child_weight': 4, 'colsample_bytree': 0.5049131988820076}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:08:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:08:37,255] Trial 16 finished with value: 0.9224137931034483 and parameters: {'max_depth': 7, 'subsample': 0.9, 'n_estimators': 4500, 'eta': 0.01, 'reg_alpha': 18, 'reg_lambda': 45, 'min_child_weight': 14, 'colsample_bytree': 0.22597846174102446}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:08:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:08:46,635] Trial 17 finished with value: 0.9353448275862069 and parameters: {'max_depth': 2, 'subsample': 0.95, 'n_estimators': 2100, 'eta': 0.09, 'reg_alpha': 9, 'reg_lambda': 5, 'min_child_weight': 8, 'colsample_bytree': 0.9694815813094735}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:08:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:09:17,178] Trial 18 finished with value: 0.9094827586206896 and parameters: {'max_depth': 8, 'subsample': 0.85, 'n_estimators': 7200, 'eta': 0.06999999999999999, 'reg_alpha': 37, 'reg_lambda': 66, 'min_child_weight': 12, 'colsample_bytree': 0.756900364766526}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:09:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:09:34,830] Trial 19 finished with value: 0.9310344827586207 and parameters: {'max_depth': 13, 'subsample': 1.0, 'n_estimators': 4600, 'eta': 0.05, 'reg_alpha': 23, 'reg_lambda': 19, 'min_child_weight': 5, 'colsample_bytree': 0.5999833040304661}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:09:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:09:46,518] Trial 20 finished with value: 0.9310344827586207 and parameters: {'max_depth': 10, 'subsample': 0.8, 'n_estimators': 1900, 'eta': 0.03, 'reg_alpha': 7, 'reg_lambda': 35, 'min_child_weight': 19, 'colsample_bytree': 0.8692659558269076}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:10:07,425] Trial 21 finished with value: 0.9482758620689655 and parameters: {'max_depth': 9, 'subsample': 0.85, 'n_estimators': 3900, 'eta': 0.060000000000000005, 'reg_alpha': 5, 'reg_lambda': 49, 'min_child_weight': 7, 'colsample_bytree': 0.38244176449498324}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:10:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:10:25,300] Trial 22 finished with value: 0.9353448275862069 and parameters: {'max_depth': 8, 'subsample': 0.9, 'n_estimators': 4100, 'eta': 0.06999999999999999, 'reg_alpha': 15, 'reg_lambda': 49, 'min_child_weight': 8, 'colsample_bytree': 0.4270100121500596}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:10:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:10:51,928] Trial 23 finished with value: 0.9482758620689655 and parameters: {'max_depth': 6, 'subsample': 0.85, 'n_estimators': 5700, 'eta': 0.060000000000000005, 'reg_alpha': 7, 'reg_lambda': 56, 'min_child_weight': 6, 'colsample_bytree': 0.2661688453819894}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:10:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:11:21,915] Trial 24 finished with value: 0.9439655172413793 and parameters: {'max_depth': 5, 'subsample': 0.75, 'n_estimators': 5500, 'eta': 0.060000000000000005, 'reg_alpha': 5, 'reg_lambda': 63, 'min_child_weight': 4, 'colsample_bytree': 0.2581973417200078}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:11:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:12:15,852] Trial 25 finished with value: 0.9224137931034483 and parameters: {'max_depth': 6, 'subsample': 0.85, 'n_estimators': 6500, 'eta': 0.05, 'reg_alpha': 1, 'reg_lambda': 74, 'min_child_weight': 2, 'colsample_bytree': 0.10144620929754672}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:12:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:12:53,023] Trial 26 finished with value: 0.9310344827586207 and parameters: {'max_depth': 6, 'subsample': 0.8, 'n_estimators': 8200, 'eta': 0.060000000000000005, 'reg_alpha': 12, 'reg_lambda': 85, 'min_child_weight': 5, 'colsample_bytree': 0.2637635661199666}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:12:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:13:19,340] Trial 27 finished with value: 0.9396551724137931 and parameters: {'max_depth': 5, 'subsample': 0.95, 'n_estimators': 5500, 'eta': 0.03, 'reg_alpha': 7, 'reg_lambda': 40, 'min_child_weight': 12, 'colsample_bytree': 0.19349511313921983}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:13:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:13:53,607] Trial 28 finished with value: 0.9310344827586207 and parameters: {'max_depth': 2, 'subsample': 0.8, 'n_estimators': 7700, 'eta': 0.05, 'reg_alpha': 12, 'reg_lambda': 22, 'min_child_weight': 9, 'colsample_bytree': 0.5480633534640331}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:13:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:14:20,593] Trial 29 finished with value: 0.9396551724137931 and parameters: {'max_depth': 7, 'subsample': 0.7, 'n_estimators': 5000, 'eta': 0.06999999999999999, 'reg_alpha': 6, 'reg_lambda': 81, 'min_child_weight': 2, 'colsample_bytree': 0.4231848398013578}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:14:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:14:47,324] Trial 30 finished with value: 0.9310344827586207 and parameters: {'max_depth': 8, 'subsample': 0.85, 'n_estimators': 6100, 'eta': 0.04, 'reg_alpha': 14, 'reg_lambda': 55, 'min_child_weight': 6, 'colsample_bytree': 0.30321039405110894}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:14:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:15:17,448] Trial 31 finished with value: 0.9396551724137931 and parameters: {'max_depth': 5, 'subsample': 0.75, 'n_estimators': 5600, 'eta': 0.060000000000000005, 'reg_alpha': 5, 'reg_lambda': 68, 'min_child_weight': 4, 'colsample_bytree': 0.15723531500322524}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:15:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:16:00,426] Trial 32 finished with value: 0.9267241379310345 and parameters: {'max_depth': 4, 'subsample': 0.7, 'n_estimators': 5100, 'eta': 0.060000000000000005, 'reg_alpha': 1, 'reg_lambda': 62, 'min_child_weight': 3, 'colsample_bytree': 0.2464594354367344}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:16:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:16:19,129] Trial 33 finished with value: 0.9396551724137931 and parameters: {'max_depth': 6, 'subsample': 0.75, 'n_estimators': 4000, 'eta': 0.06999999999999999, 'reg_alpha': 8, 'reg_lambda': 52, 'min_child_weight': 4, 'colsample_bytree': 0.2911233965858202}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:16:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:16:52,649] Trial 34 finished with value: 0.9310344827586207 and parameters: {'max_depth': 3, 'subsample': 0.8, 'n_estimators': 6200, 'eta': 0.05, 'reg_alpha': 4, 'reg_lambda': 63, 'min_child_weight': 8, 'colsample_bytree': 0.17503272336443765}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:16:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:17:06,430] Trial 35 finished with value: 0.9267241379310345 and parameters: {'max_depth': 5, 'subsample': 0.75, 'n_estimators': 3000, 'eta': 0.08, 'reg_alpha': 20, 'reg_lambda': 93, 'min_child_weight': 6, 'colsample_bytree': 0.3974628873775006}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:17:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:17:29,222] Trial 36 finished with value: 0.9267241379310345 and parameters: {'max_depth': 7, 'subsample': 0.65, 'n_estimators': 5600, 'eta': 0.09, 'reg_alpha': 25, 'reg_lambda': 13, 'min_child_weight': 10, 'colsample_bytree': 0.4745077094658726}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:17:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:17:47,506] Trial 37 finished with value: 0.9396551724137931 and parameters: {'max_depth': 3, 'subsample': 0.95, 'n_estimators': 4000, 'eta': 0.060000000000000005, 'reg_alpha': 9, 'reg_lambda': 48, 'min_child_weight': 3, 'colsample_bytree': 0.5494285710002622}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:17:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:18:16,828] Trial 38 finished with value: 0.9267241379310345 and parameters: {'max_depth': 4, 'subsample': 0.9, 'n_estimators': 6900, 'eta': 0.03, 'reg_alpha': 29, 'reg_lambda': 42, 'min_child_weight': 6, 'colsample_bytree': 0.679678343492375}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:18:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:18:25,874] Trial 39 finished with value: 0.9353448275862069 and parameters: {'max_depth': 15, 'subsample': 0.7, 'n_estimators': 1700, 'eta': 0.06999999999999999, 'reg_alpha': 14, 'reg_lambda': 29, 'min_child_weight': 7, 'colsample_bytree': 0.892469077266974}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:18:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:18:51,034] Trial 40 finished with value: 0.9396551724137931 and parameters: {'max_depth': 10, 'subsample': 0.65, 'n_estimators': 3600, 'eta': 0.04, 'reg_alpha': 3, 'reg_lambda': 59, 'min_child_weight': 9, 'colsample_bytree': 0.30889869881421084}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:18:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:19:16,922] Trial 41 finished with value: 0.9396551724137931 and parameters: {'max_depth': 5, 'subsample': 0.95, 'n_estimators': 5400, 'eta': 0.03, 'reg_alpha': 7, 'reg_lambda': 37, 'min_child_weight': 12, 'colsample_bytree': 0.16423504937112152}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:19:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:19:39,593] Trial 42 finished with value: 0.9396551724137931 and parameters: {'max_depth': 4, 'subsample': 1.0, 'n_estimators': 4300, 'eta': 0.02, 'reg_alpha': 6, 'reg_lambda': 54, 'min_child_weight': 11, 'colsample_bytree': 0.2027739301345874}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:19:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:20:17,039] Trial 43 finished with value: 0.9224137931034483 and parameters: {'max_depth': 6, 'subsample': 0.95, 'n_estimators': 5800, 'eta': 0.01, 'reg_alpha': 3, 'reg_lambda': 41, 'min_child_weight': 17, 'colsample_bytree': 0.13367089575081387}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:20:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:20:40,126] Trial 44 finished with value: 0.9353448275862069 and parameters: {'max_depth': 5, 'subsample': 0.95, 'n_estimators': 5200, 'eta': 0.05, 'reg_alpha': 10, 'reg_lambda': 73, 'min_child_weight': 13, 'colsample_bytree': 0.20642860568753738}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:20:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:20:53,743] Trial 45 finished with value: 0.9267241379310345 and parameters: {'max_depth': 9, 'subsample': 0.85, 'n_estimators': 2900, 'eta': 0.04, 'reg_alpha': 17, 'reg_lambda': 32, 'min_child_weight': 10, 'colsample_bytree': 0.26877064281387786}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:20:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:21:10,563] Trial 46 finished with value: 0.9310344827586207 and parameters: {'max_depth': 8, 'subsample': 0.9, 'n_estimators': 3700, 'eta': 0.060000000000000005, 'reg_alpha': 12, 'reg_lambda': 50, 'min_child_weight': 5, 'colsample_bytree': 0.3505153403412542}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:21:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:21:35,855] Trial 47 finished with value: 0.9094827586206896 and parameters: {'max_depth': 3, 'subsample': 1.0, 'n_estimators': 6600, 'eta': 0.02, 'reg_alpha': 50, 'reg_lambda': 25, 'min_child_weight': 16, 'colsample_bytree': 0.6133592530719125}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:22:32,052] Trial 48 finished with value: 0.9396551724137931 and parameters: {'max_depth': 7, 'subsample': 0.85, 'n_estimators': 10000, 'eta': 0.03, 'reg_alpha': 5, 'reg_lambda': 46, 'min_child_weight': 3, 'colsample_bytree': 0.774585730910125}. Best is trial 3 with value: 0.9482758620689655.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:21: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 1.0, 0.05),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:22: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  'n_estimators': trial.suggest_int('n_estimators', 1000, 10000, 100),\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5148\\487667124.py:23: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  'eta': trial.suggest_discrete_uniform('eta', 0.01, 0.1, 0.01),\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2024-07-03 13:23:02,942] Trial 49 finished with value: 0.9267241379310345 and parameters: {'max_depth': 6, 'subsample': 0.9, 'n_estimators': 7300, 'eta': 0.060000000000000005, 'reg_alpha': 21, 'reg_lambda': 13, 'min_child_weight': 8, 'colsample_bytree': 0.33031159567436286}. Best is trial 3 with value: 0.9482758620689655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBClassifier: {'max_depth': 6, 'subsample': 0.95, 'n_estimators': 3000, 'eta': 0.05, 'reg_alpha': 5, 'reg_lambda': 19, 'min_child_weight': 6, 'colsample_bytree': 0.6699718653592507}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 13:23:19,431] A new study created in memory with name: no-name-11c7961d-48f7-4517-862f-3ce68af1ac71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as best_xgbclassifier.pkl\n",
      "Test accuracy for XGBClassifier: 0.9661\n",
      "Training CatBoostClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 13:23:58,327] Trial 0 finished with value: 0.9353448275862069 and parameters: {'n_estimators': 405, 'depth': 6, 'learning_rate': 0.23287424574001783}. Best is trial 0 with value: 0.9353448275862069.\n",
      "[I 2024-07-03 13:24:05,016] Trial 1 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 225, 'depth': 5, 'learning_rate': 0.2916172117608551}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:15,936] Trial 2 finished with value: 0.9310344827586207 and parameters: {'n_estimators': 434, 'depth': 5, 'learning_rate': 0.2988976432792824}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:19,232] Trial 3 finished with value: 0.9267241379310345 and parameters: {'n_estimators': 214, 'depth': 1, 'learning_rate': 0.12054150297464668}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:27,743] Trial 4 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 94, 'depth': 10, 'learning_rate': 0.20024228635400604}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:34,224] Trial 5 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 206, 'depth': 5, 'learning_rate': 0.12991358408151157}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:40,585] Trial 6 finished with value: 0.9267241379310345 and parameters: {'n_estimators': 388, 'depth': 2, 'learning_rate': 0.20807041211215502}. Best is trial 1 with value: 0.9439655172413793.\n",
      "[I 2024-07-03 13:24:53,926] Trial 7 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 257, 'depth': 8, 'learning_rate': 0.11305977375747792}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:25:28,704] Trial 8 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 422, 'depth': 10, 'learning_rate': 0.07575985987622856}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:25:31,447] Trial 9 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 139, 'depth': 2, 'learning_rate': 0.24199110178910505}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:25:54,602] Trial 10 finished with value: 0.9353448275862069 and parameters: {'n_estimators': 319, 'depth': 8, 'learning_rate': 0.010045854836337104}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:26:21,209] Trial 11 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 322, 'depth': 10, 'learning_rate': 0.07049454565689398}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:26:48,903] Trial 12 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 474, 'depth': 8, 'learning_rate': 0.07495681556043976}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:27:03,388] Trial 13 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 313, 'depth': 8, 'learning_rate': 0.06334292550071702}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:27:32,720] Trial 14 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 498, 'depth': 9, 'learning_rate': 0.1493295514167557}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:27:43,973] Trial 15 finished with value: 0.9353448275862069 and parameters: {'n_estimators': 266, 'depth': 7, 'learning_rate': 0.017320752892577063}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:28:14,430] Trial 16 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 364, 'depth': 10, 'learning_rate': 0.10833475266986686}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:28:28,325] Trial 17 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 167, 'depth': 7, 'learning_rate': 0.16851101611275207}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:28:47,096] Trial 18 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 280, 'depth': 9, 'learning_rate': 0.09135489521420977}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:29:12,940] Trial 19 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 437, 'depth': 9, 'learning_rate': 0.049155737120384194}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:29:21,113] Trial 20 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 347, 'depth': 4, 'learning_rate': 0.04532247453825222}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:29:45,065] Trial 21 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 273, 'depth': 10, 'learning_rate': 0.09171481453529583}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:30:11,888] Trial 22 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 333, 'depth': 10, 'learning_rate': 0.03493539242433029}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:30:44,966] Trial 23 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 384, 'depth': 9, 'learning_rate': 0.08535148356207108}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:30:54,511] Trial 24 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 249, 'depth': 7, 'learning_rate': 0.14175797745314145}. Best is trial 7 with value: 0.9482758620689655.\n",
      "[I 2024-07-03 13:31:07,886] Trial 25 finished with value: 0.9525862068965517 and parameters: {'n_estimators': 293, 'depth': 8, 'learning_rate': 0.11161795034583433}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:31:13,932] Trial 26 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 184, 'depth': 6, 'learning_rate': 0.16997319306302214}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:31:20,123] Trial 27 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 120, 'depth': 8, 'learning_rate': 0.11074278634181534}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:31:37,040] Trial 28 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 452, 'depth': 7, 'learning_rate': 0.10368345411406742}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:31:49,899] Trial 29 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 405, 'depth': 6, 'learning_rate': 0.16578310543498248}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:31:54,089] Trial 30 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 57, 'depth': 9, 'learning_rate': 0.1320151232536887}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:32:20,507] Trial 31 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 299, 'depth': 10, 'learning_rate': 0.06163758856158566}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:32:39,678] Trial 32 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 242, 'depth': 8, 'learning_rate': 0.07556227348236952}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:33:02,225] Trial 33 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 352, 'depth': 9, 'learning_rate': 0.03603959033060199}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:33:09,129] Trial 34 finished with value: 0.9353448275862069 and parameters: {'n_estimators': 294, 'depth': 4, 'learning_rate': 0.11758636056304218}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:33:42,321] Trial 35 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 412, 'depth': 10, 'learning_rate': 0.0677351496382681}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:34:00,053] Trial 36 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 227, 'depth': 9, 'learning_rate': 0.08837421043727528}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:34:30,195] Trial 37 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 372, 'depth': 10, 'learning_rate': 0.1295996955661754}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:34:40,563] Trial 38 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 334, 'depth': 6, 'learning_rate': 0.18908654920294174}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:34:52,240] Trial 39 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 176, 'depth': 8, 'learning_rate': 0.27337962369225477}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:34:58,123] Trial 40 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 258, 'depth': 4, 'learning_rate': 0.10220448264089592}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:35:27,504] Trial 41 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 494, 'depth': 9, 'learning_rate': 0.14219451068392278}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:36:10,640] Trial 42 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 494, 'depth': 9, 'learning_rate': 0.15479913637081993}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:36:32,406] Trial 43 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 456, 'depth': 8, 'learning_rate': 0.19128714319805626}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:37:06,534] Trial 44 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 411, 'depth': 10, 'learning_rate': 0.12068081142066818}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:37:32,986] Trial 45 finished with value: 0.9482758620689655 and parameters: {'n_estimators': 476, 'depth': 7, 'learning_rate': 0.14439964446233433}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:37:42,882] Trial 46 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 203, 'depth': 8, 'learning_rate': 0.2222442034571801}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:38:18,975] Trial 47 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 426, 'depth': 10, 'learning_rate': 0.05389954269785447}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:38:38,394] Trial 48 finished with value: 0.9439655172413793 and parameters: {'n_estimators': 311, 'depth': 9, 'learning_rate': 0.07857786015723911}. Best is trial 25 with value: 0.9525862068965517.\n",
      "[I 2024-07-03 13:38:46,779] Trial 49 finished with value: 0.9396551724137931 and parameters: {'n_estimators': 287, 'depth': 5, 'learning_rate': 0.10129502187228226}. Best is trial 25 with value: 0.9525862068965517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoostClassifier: {'n_estimators': 293, 'depth': 8, 'learning_rate': 0.11161795034583433}\n",
      "Model saved as best_catboostclassifier.pkl\n",
      "Test accuracy for CatBoostClassifier: 0.9831\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "        \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "        \"XGBClassifier\": XGBClassifier(),\n",
    "        \"CatBoostClassifier\": CatBoostClassifier()\n",
    "    }\n",
    "\n",
    "for classifier_name in classifiers.keys():\n",
    "    print(f\"Training {classifier_name}...\")\n",
    "    model, best_params = train_and_tune_model(X_train, y_train, X_val, y_val, classifier_name)\n",
    "    print(f\"Best parameters for {classifier_name}: {best_params}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    model_filename = f\"best_{classifier_name.replace(' ', '_').lower()}.pkl\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test accuracy for {classifier_name}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for K-Neighbors Classifier: 0.8983\n",
      "Test accuracy for Decision Tree: 1.0000\n",
      "Test accuracy for Random Forest Classifier: 1.0000\n",
      "Test accuracy for XGBClassifier: 0.9661\n",
      "Test accuracy for CatBoostClassifier: 0.9831\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "        \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "        \"XGBClassifier\": XGBClassifier(),\n",
    "        \"CatBoostClassifier\": CatBoostClassifier()}\n",
    "# Load the saved models and evaluate them on the test set\n",
    "for classifier_name in classifiers:\n",
    "    # Load the model\n",
    "    model_filename = f\"best_{classifier_name.replace(' ', '_').lower()}.pkl\"\n",
    "    model = joblib.load(model_filename)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    save_test_result(model_name= classifier_name, score = test_accuracy)\n",
    "    print(f\"Test accuracy for {classifier_name}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "\n",
    "    X_train_array= X_train.values\n",
    "    X_val_array=X_val.values\n",
    "    X_test_array=X_test.values\n",
    "    y_train_array= y_train.values\n",
    "    y_val_array=y_val.values \n",
    "    y_test_array= y_test.values\n",
    "\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    X_train_array = sc.fit_transform(X_train_array)\n",
    "    X_val_array= sc.transform(X_val_array)\n",
    "    X_test_array = sc.transform(X_test_array)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_array, dtype=torch.long)  # Assuming class labels are integers\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_val_tensor = torch.tensor(X_val_array, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_array, dtype=torch.long)  # Assuming class labels are integers\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_array, dtype=torch.long)  # Assuming class labels are integers\n",
    "\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, X_test_tensor, y_test_tensor = scale(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2,hidden_dim3, output_dim,drop_out_prob):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.Dropout = nn.Dropout(drop_out_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.Dropout(self.relu(self.fc1(x)))\n",
    "        out = self.Dropout(self.relu(self.fc2(out)))\n",
    "        out = self.Dropout(self.relu(self.fc3(out)))\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim1 = 64  # You can change this to experiment with different architectures\n",
    "hidden_dim2 = 32 \n",
    "hidden_dim3 = 16\n",
    "output_dim = len(np.unique(y_train))  # Number of unique classes\n",
    "model = MLP(input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim, drop_out_prob=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20  # Number of epochs to wait if validation loss stops improving\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # Counter for early stopping\n",
    "best_model = None  # Variable to store the best model state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 1.3740, Val Loss: 1.3627\n",
      "Epoch [20/500], Loss: 1.3181, Val Loss: 1.3008\n",
      "Epoch [30/500], Loss: 1.2295, Val Loss: 1.1997\n",
      "Epoch [40/500], Loss: 1.1041, Val Loss: 1.0491\n",
      "Epoch [50/500], Loss: 0.9439, Val Loss: 0.8683\n",
      "Epoch [60/500], Loss: 0.7671, Val Loss: 0.7080\n",
      "Epoch [70/500], Loss: 0.6727, Val Loss: 0.5992\n",
      "Epoch [80/500], Loss: 0.5827, Val Loss: 0.5189\n",
      "Epoch [90/500], Loss: 0.5593, Val Loss: 0.4642\n",
      "Epoch [100/500], Loss: 0.4802, Val Loss: 0.4309\n",
      "Epoch [110/500], Loss: 0.4592, Val Loss: 0.4101\n",
      "Epoch [120/500], Loss: 0.4458, Val Loss: 0.3950\n",
      "Epoch [130/500], Loss: 0.4320, Val Loss: 0.3840\n",
      "Epoch [140/500], Loss: 0.4243, Val Loss: 0.3751\n",
      "Epoch [150/500], Loss: 0.3872, Val Loss: 0.3670\n",
      "Epoch [160/500], Loss: 0.3956, Val Loss: 0.3596\n",
      "Epoch [170/500], Loss: 0.3819, Val Loss: 0.3520\n",
      "Epoch [180/500], Loss: 0.3872, Val Loss: 0.3448\n",
      "Epoch [190/500], Loss: 0.3738, Val Loss: 0.3379\n",
      "Epoch [200/500], Loss: 0.3667, Val Loss: 0.3323\n",
      "Epoch [210/500], Loss: 0.3700, Val Loss: 0.3268\n",
      "Epoch [220/500], Loss: 0.3181, Val Loss: 0.3206\n",
      "Epoch [230/500], Loss: 0.3124, Val Loss: 0.3175\n",
      "Epoch [240/500], Loss: 0.3211, Val Loss: 0.3119\n",
      "Epoch [250/500], Loss: 0.3153, Val Loss: 0.3040\n",
      "Epoch [260/500], Loss: 0.3081, Val Loss: 0.2980\n",
      "Epoch [270/500], Loss: 0.2845, Val Loss: 0.2932\n",
      "Epoch [280/500], Loss: 0.3048, Val Loss: 0.2892\n",
      "Epoch [290/500], Loss: 0.2827, Val Loss: 0.2866\n",
      "Epoch [300/500], Loss: 0.2656, Val Loss: 0.2816\n",
      "Epoch [310/500], Loss: 0.2725, Val Loss: 0.2789\n",
      "Epoch [320/500], Loss: 0.2588, Val Loss: 0.2739\n",
      "Epoch [330/500], Loss: 0.2735, Val Loss: 0.2738\n",
      "Epoch [340/500], Loss: 0.2658, Val Loss: 0.2705\n",
      "Epoch [350/500], Loss: 0.2649, Val Loss: 0.2691\n",
      "Epoch [360/500], Loss: 0.2499, Val Loss: 0.2672\n",
      "Epoch [370/500], Loss: 0.2504, Val Loss: 0.2637\n",
      "Epoch [380/500], Loss: 0.2437, Val Loss: 0.2643\n",
      "Epoch [390/500], Loss: 0.2454, Val Loss: 0.2597\n",
      "Epoch [400/500], Loss: 0.2382, Val Loss: 0.2570\n",
      "Epoch [410/500], Loss: 0.2380, Val Loss: 0.2582\n",
      "Epoch [420/500], Loss: 0.2242, Val Loss: 0.2555\n",
      "Epoch [430/500], Loss: 0.2138, Val Loss: 0.2584\n",
      "Early stopping at epoch 436 with validation loss: 0.2597\n",
      "Best model loaded.\n",
      "Best model saved to 'best_MLP_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping and model saving\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    # Early stopping logic and model saving\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the best model state_dict\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} with validation loss: {val_loss.item():.4f}')\n",
    "            break\n",
    "\n",
    "# Load the best model state_dict back into the model\n",
    "if best_model is not None:\n",
    "    model.load_state_dict(best_model)\n",
    "    print(\"Best model loaded.\")\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(model.state_dict(), 'best_MLP_model.pth')\n",
    "print(\"Best model saved to 'best_MLP_model.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.92%\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_dim, hidden_dim1,hidden_dim2,hidden_dim3, output_dim, drop_out_prob=0.2)\n",
    "\n",
    "model.load_state_dict(torch.load('best_MLP_model.pth'))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "correct = (predicted == y_test_tensor).sum().item()\n",
    "accuracy = correct / len(y_test_tensor) * 100\n",
    "save_test_result(model_name= 'MLP', score= accuracy)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAB PFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have to download the TabPFN, as there is no checkpoint at  E:\\Project Thesis\\Data set\\final\\venv\\Lib\\site-packages\\tabpfn\\models_diff/prior_diff_real_checkpoint_n_0_epoch_100.cpkt\n",
      "It has about 100MB, so this might take a moment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9491525423728814\n"
     ]
    }
   ],
   "source": [
    "classifier = TabPFNClassifier(device='cpu',N_ensemble_configurations=64)\n",
    "classifier.fit(X_train, y_train, overwrite_warning=True)\n",
    "y_eval, p_eval = classifier.predict(X_test, return_winning_probability=True)\n",
    "\n",
    "print('Accuracy', accuracy_score(y_test, y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9224137931034483\n"
     ]
    }
   ],
   "source": [
    "y_eval, p_eval = classifier.predict(X_val, return_winning_probability=True)\n",
    "\n",
    "save_test_result(model_name='Tab pfn', score =accuracy_score(y_val, y_eval) )\n",
    "\n",
    "print('Accuracy', accuracy_score(y_val, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 D CNN for Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from https://github.com/baosenguo/Kaggle-MoA-2nd-Place-Solution/blob/main/training/1d-cnn-train.ipynb\n",
    "\n",
    "class OneD_Model(nn.Module):\n",
    "        def __init__(self, num_features, num_targets, hidden_size):\n",
    "            super(OneD_Model, self).__init__()\n",
    "            cha_1 = 256\n",
    "            cha_2 = 512\n",
    "            cha_3 = 512\n",
    "\n",
    "            cha_1_reshape = int(hidden_size/cha_1)\n",
    "            cha_po_1 = int(hidden_size/cha_1/2)\n",
    "            cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
    "\n",
    "            self.cha_1 = cha_1\n",
    "            self.cha_2 = cha_2\n",
    "            self.cha_3 = cha_3\n",
    "            self.cha_1_reshape = cha_1_reshape\n",
    "            self.cha_po_1 = cha_po_1\n",
    "            self.cha_po_2 = cha_po_2\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "            self.dropout1 = nn.Dropout(0.1)\n",
    "            self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "\n",
    "            self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
    "            self.dropout_c1 = nn.Dropout(0.1)\n",
    "            self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "            self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
    "\n",
    "            self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2 = nn.Dropout(0.1)\n",
    "            self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_1 = nn.Dropout(0.3)\n",
    "            self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "\n",
    "            self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
    "            self.dropout_c2_2 = nn.Dropout(0.2)\n",
    "            self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
    "\n",
    "            self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "            self.flt = nn.Flatten()\n",
    "\n",
    "            self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
    "            self.dropout3 = nn.Dropout(0.2)\n",
    "            self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            x = self.batch_norm1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "            x = x.reshape(x.shape[0],self.cha_1,\n",
    "                          self.cha_1_reshape)\n",
    "\n",
    "            x = self.batch_norm_c1(x)\n",
    "            x = self.dropout_c1(x)\n",
    "            x = F.relu(self.conv1(x))\n",
    "\n",
    "            x = self.ave_po_c1(x)\n",
    "\n",
    "            x = self.batch_norm_c2(x)\n",
    "            x = self.dropout_c2(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x_s = x\n",
    "\n",
    "            x = self.batch_norm_c2_1(x)\n",
    "            x = self.dropout_c2_1(x)\n",
    "            x = F.relu(self.conv2_1(x))\n",
    "\n",
    "            x = self.batch_norm_c2_2(x)\n",
    "            x = self.dropout_c2_2(x)\n",
    "            x = F.relu(self.conv2_2(x))\n",
    "            x =  x * x_s\n",
    "\n",
    "            x = self.max_po_c2(x)\n",
    "\n",
    "            x = self.flt(x)\n",
    "\n",
    "            x = self.batch_norm3(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.dense3(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Project Thesis\\Data set\\final\\venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_targets = len(np.unique(y_train))\n",
    "hidden_size = 4096\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2 = OneD_Model(num_features, num_targets, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 10  # Number of epochs to wait if validation loss stops improving\n",
    "best_val_loss = float('inf')\n",
    "counter = 0  # Counter for early stopping\n",
    "best_model = None  # Variable to store the best model state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.2326, Val Loss: 1.0660\n",
      "Epoch [20/500], Loss: 0.1875, Val Loss: 0.3627\n",
      "Epoch [30/500], Loss: 0.1723, Val Loss: 0.2642\n",
      "Epoch [40/500], Loss: 0.1578, Val Loss: 0.2926\n",
      "Early stopping at epoch 41 with validation loss: 0.3150\n",
      "Best model loaded.\n",
      "Best model saved to 'best_1D_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model2(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model2(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # Save the best model state_dict\n",
    "        best_model = model2.state_dict()\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} with validation loss: {val_loss.item():.4f}')\n",
    "            break\n",
    "\n",
    "# Load the best model state_dict back into the model\n",
    "if best_model is not None:\n",
    "    model2.load_state_dict(best_model)\n",
    "    print(\"Best model loaded.\")\n",
    "\n",
    "# Save the best model to a file\n",
    "torch.save(model2.state_dict(), 'best_1D_model.pth')\n",
    "print(\"Best model saved to 'best_1D_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.92%\n"
     ]
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('best_1D_model.pth'))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model2(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "correct = (predicted == y_test_tensor).sum().item()\n",
    "accuracy = correct / len(y_test_tensor) * 100\n",
    "save_test_result(model_name='1D CNN', score=accuracy)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame.from_dict(test_result, orient='index', columns=['score' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>K-Neighbors Classifier</th>\n",
       "      <td>0.898305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.966102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.983051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>94.915254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tab pfn</th>\n",
       "      <td>0.922414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1D CNN</th>\n",
       "      <td>94.915254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              score\n",
       "K-Neighbors Classifier     0.898305\n",
       "Decision Tree              1.000000\n",
       "Random Forest Classifier   1.000000\n",
       "XGBClassifier              0.966102\n",
       "CatBoostClassifier         0.983051\n",
       "MLP                       94.915254\n",
       "Tab pfn                    0.922414\n",
       "1D CNN                    94.915254"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
